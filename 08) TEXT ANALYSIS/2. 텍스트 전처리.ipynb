{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e861e16",
   "metadata": {},
   "source": [
    "# 텍스트 사전 준비 작업 - 텍스트 정규화\n",
    "#### 택스트 자체를 바로 피처로 만들 수는 없다. 이를 위해서 텍스트를 가공해야 한다.\n",
    "#### 텍스트 정규화는 텍스트를 머신러닝 알고리즘이나 NLP application에 입력 데이터로 사용하기 위해 다양한 텍스트 사전 작업을 수행하는 것이다.\n",
    "#### 텍스트 정규화 작업은 아래와 같이 분류할 수 있다.\n",
    "#### - 클렌징(Cleansing)\n",
    "#### - 토큰화(Tokenization)\n",
    "#### - 필터링/stopword 제거/철자 수정\n",
    "#### - 어간 추출(Stemming)\n",
    "#### - 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f992609",
   "metadata": {},
   "source": [
    "## 클렌징\n",
    "#### 텍스트에서 분석에 방해가 되는 불필요한 문자, 기호 등을 사전에 제거하는 작업이다. 예를 들어, HTML / XML 태그나 특정 기호 등을 사전에 제거한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7224bc5",
   "metadata": {},
   "source": [
    "## 텍스트 토큰화\n",
    "#### 토큰화의 유형은 문서에서 문장을 분리하는 문장 토큰화와 문장어서 단어를 분리하는 단어 토큰화로 나뉜다.\n",
    "\n",
    "### 문장 토큰화\n",
    "#### 문장 토큰화는 문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리하는 것이 일반적이다. 또한 정규 표현식에 따른 문장 토큰화도 가능하다.\n",
    "#### 아래에 NLTK에서 많이 사용되는 sent_tokenize를 이용해 토큰화를 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b93b9ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around usz here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\junseok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text_sample = 'The Matrix is everywhere its all around usz here even in this room. \\\n",
    "You can see it out your window or on your television. \\\n",
    "You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b51286",
   "metadata": {},
   "source": [
    "#### 결과는 각각의 문장으로 구성된 list 객체이고, 3개의 문장으로 분리되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eacabfa",
   "metadata": {},
   "source": [
    "### 단어 토큰화\n",
    "#### 단어 토큰화는 문장을 단어로 토큰화하는 것이다. 기본적으로 공백, 콤바(,), 마침표(.), 개행문자 등으로 단어를 분리하지만 정규 표현식을 이용해 다양한 유형으로 토큰화를 수행할 수 있다.\n",
    "#### 마침표나 개행문자와 같이 문장을 분리하는 구분자를 이용해 단어를 토큰화할 때는 문장 토큰화를 사용하지 않고 단어 토큰화만 사용해도 충분하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "789d8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', '니s', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around 니s, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7a051",
   "metadata": {},
   "source": [
    "#### sent_tokenize와 word_tokenize를 조합해 문서에 대해 모든 단어를 토큰화를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ab0d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'usz', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0b0ca",
   "metadata": {},
   "source": [
    "#### 문장을 단어별로 하나씩 토큰화를 할 경우 문맥적인 의미는 무시된다. 이러한 문제를 해결하기 위해 n-gram이라는 연속된 n개의 단어를 하나의 토큰화 단위로 분리한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a325f64",
   "metadata": {},
   "source": [
    "### stopwords 제거\n",
    "#### stopwords는 분석에 큰 의미가 없는 단어를 말한다. 예를 들어, is the, a, will 등을 나타낸다.\n",
    "#### 해당 단어들은 문법적인 특성으로 빈번하게 나오기 때문에 오히려 중요한 단어로 인식되어 분석에 방해가 된다. 그래서 nltk의 stopword를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57bfe623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\junseok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0734a76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 개수 :  179\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 개수 : ', len(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6ad30",
   "metadata": {},
   "source": [
    "#### 바로 위 예제에서 3개의 문장별로 단어를 토큰화된 2차원 리스트에서 stopwords를 제외하여 표시해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a636942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'usz', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    filtered = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered.append(word)\n",
    "    all_tokens.append(filtered)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ec6a8",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization\n",
    "#### 많은 어어에서 문법적인 요소에 따라 단어가 다양하게 변한다. 예를 들어, 영어의 경우 과거/현재, 3인칭 단수 여부, 진행형 등으로 원래 단어가 변한다.\n",
    "#### 두 기능 모두 원형 단어를 찾는다는 목적은 유사하지만, Lemmatization이 Stemming보다 정교하며 의미론적인 기반에서 단어의 원형을 찾는다.\n",
    "#### NLTK에서 다양한 stemmer를 제공하고, 아래에 LancasterStemmer를 이용해 Stemmer를 살펴본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9d82312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked')) \n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused')) \n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest')) \n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b8d61b",
   "metadata": {},
   "source": [
    "#### work의 경우 진행형, 3인칭 단수, 과거형 모두 원형 단어로 work로 인식한다.\n",
    "#### 하지만 amuse의 경우 amus를 원형 단어로 인식하고, happy/fancy의 경우 정확한 원형을 찾지 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927464f",
   "metadata": {},
   "source": [
    "#### 이번에는 WordNetLemmatizer를 이용해 Lemmatization을 수행할 것이다.\n",
    "#### Lemmatization은 보다 정확한 원형 단어 추출을 위해서 '품사'를 입력해줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a453efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\junseok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v')) \n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a')) \n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a985e034",
   "metadata": {},
   "source": [
    "#### 정확하게 원형 추출이 완료된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
