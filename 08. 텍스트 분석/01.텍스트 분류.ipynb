{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8bbe55-f1e1-4d09-bd1a-0c86315712c5",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a10959-7c4a-4159-bc34-d93d45704fc5",
   "metadata": {},
   "source": [
    "## Text Tokenization\n",
    "nltk 패키지를 사용하며 nltk.download()를 통해 필요 데이터를 다운받는다.\n",
    "text_sample에 3개의 문장을 담아 텍스트 분류를 진행한다.\n",
    "\n",
    "텍스트 분류를 진행하려면\n",
    "1. 텍스트 전처리\n",
    "2. 텍스트 토큰화\n",
    "3. ML 모델 수립\n",
    "과정을 거친다.\n",
    "\n",
    "### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e361cc-597e-467e-bf7e-d891186c6a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/minji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5381ec58-ac21-4438-b01c-4c1d5e1f409c",
   "metadata": {},
   "source": [
    "### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f54af838-2a3f-41fd-a645-30b397912b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7c4fe3-3c14-4e5b-a0be-6e80caddbd09",
   "metadata": {},
   "source": [
    "### 여러 개의 문장에서 단어 토큰화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98453fa1-a55c-4547-bf43-0e950ea8c028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d731c7-3a6a-49c4-a42d-b288493fe40a",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "문맥에서 의미가 없는 단어들은 제거해준다.\n",
    "영어의 stop words 중에서 20개만 출력해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b3335b-c4af-4b92-8834-a4a7224e4892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop words 갯수: 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17711a-69ef-4d5d-8f99-583a9995cbcc",
   "metadata": {},
   "source": [
    "이후 text_sample에서 분리된 word_tokens를 stop words제거를 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7466d7fb-6bc2-44f5-9f0b-e7fce5bb985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb874afe-f466-4245-8550-5c7ebd36f921",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization\n",
    "stemming과 lemmatization은 단어의 원형을 찾는 과정이다.    \n",
    "stemming과 lemmatization의 차이는 의미를 가진 원형을 찾는지 단순히 공통된 단어를 찾아내는지에 따라 차이가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5159c3-f585-4500-b5d5-b591ae9841ff",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0761e5-588c-488b-bb92-6643947b87c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a053d0-ec1c-4e9c-b297-36fa3f46ee47",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efe0b161-d6a0-4b96-8dea-450fff8c3821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/minji/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19b0f1e-4755-4db6-944c-e66070b99c1d",
   "metadata": {},
   "source": [
    "위의 결과에서 볼 수 있듯이 amusing, amuses, amused의 원형을 찾을 때,    \n",
    "stemming은 단순히 공통된 amus만 찾아내지만 lemmatization은 정확한 의미를 가진 amuse를 찾아내었다.    \n",
    "그렇기때문에 대량의 데이터를 변환한다면 lemmatization이 정확하지만 stemming에 비해 다소 느리다는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5c38b-d182-4478-b3a3-3390a2a7db6b",
   "metadata": {},
   "source": [
    "## Bag of Words(BOW)\n",
    "머신러닝 알고리즘은 숫자형 feature를 데이터로 입력받아 동작하기 때문에 숫자형인 벡터로 변환하여 주는 과정을 Feature Vectorization이라고 한다.    \n",
    "BOW는 Feature Vectorization중 하나의 기법으로 Bag of Words란 여러 단어들을 bag 안에 넣어 섞는다는 의미이다.    \n",
    "BOW는 단어의 순서를 고려하지 않기 때문에 문맥적인 해석을 처리하기 어렵고 희소 행렬 형태의 데이터 세트가 만들어지기 쉽다.    \n",
    "    \n",
    "이 희소 행렬 형태의 데이터를 처리하기 위해 COO, CSR 기법이 사용된다.    \n",
    "### COO 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b430d9-052e-4c38-af1b-73ae8ed18b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array( [ [ 3, 0, 1 ], [0, 2, 0 ] ] )\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))\n",
    "\n",
    "\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7393fb59-eefc-4b33-9309-d89bfa2dee10",
   "metadata": {},
   "source": [
    "COO 형식의 단점으로는 행과 열의 위치 정보를 저장할때 행의 데이터에서 같은 데이터가 여러번 저장된다는 것이다.    \n",
    "이러한 문제를 보완하기 위해 CSR 형식이 생겼다.\n",
    "\n",
    "### CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4e5640-f78d-4c41-ac13-b52ee39ffe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a2caa-18ca-4b38-8e40-7d258beec754",
   "metadata": {},
   "source": [
    "행의 데이터로 [0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5]가 저장되어 있을 경우 0이 2번 1이 5번 2가 2번으로 같은 데이터가 여러번 저장된 것을 볼 수 있는데,    \n",
    "CSR에선 0이 처음 나타나는 0번째 인덱스, 1이 처음으로 나타나는 2번째 인덱스, 2가 처음으로 나타나는 7번째 인덱스를 저장한 후    \n",
    "맨 마지막에는 행 데이터의 총 길이를 같이 넣어준다.\n",
    "\n",
    "이 코드를 coo_matrix(), csr_matrix() 함수를 사용하여 한번에 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9a91278-84f5-4fff-8b83-375e577130f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cbc31-7125-4199-b12c-541be39a09cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
