{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6b410a",
   "metadata": {},
   "source": [
    "# chapter 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f598f7",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "**결정 트리**는 **ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘**이다. <br>\n",
    "**데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리 기반의 분류 규칙을 만드는 것**이다. <br>\n",
    "\n",
    "**결정 트리의 구조** <br>\n",
    "**규칙 노드(Decision Node)** 로 표시된 **노드는 규칙 조건이 되는 것**이고 **리프 노드(Leaf Node)** 로 표시된 노드는 **결정된 클래스 값**이다. <br>\n",
    "**새로운 규칙 조건마다 서브 트리가 생성**된다. <br>\n",
    "데이터 세트에 피처가 있고 이러한 **피처가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어진다.** <br>\n",
    "많은 규칙이 있다는 것은 더욱 복잡해진다는 이야기이고, 이는 곧 **과적합**으로 이어진다. <br>\n",
    "트리의 깊이가 깊어질수록 결정트리의 예측 성능이 저하될 가능성이 높다. <br>\n",
    "\n",
    "<img src = 'image/Decision Tree Structure.jpg' alt = 'Decision Tree Structure' width='700' height='700'>\n",
    "\n",
    "**가능한 적은 결정 노드로 높은 예측 정확도**를 가지려면 **데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙**이 정해져야 한다. <br>\n",
    "**어떻게 트리를 분할할 것인가가 중요**한데 **최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것**이 필요하다. <br>\n",
    "\n",
    "데이터 세트의 균일도는 데이터를 구분하는데 필요한 정보의 양에 영향을 미친다. <br>\n",
    "**결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다.** <br>\n",
    "정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트 쪼개는 방식을 자식 트리로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다. <br>\n",
    "정보 균일도를 측정하는 대표적인 방법은 **엔트로피를 이용한 정보 이득(Information Gain)지수**와 **지니 계수**가 있다. <br>\n",
    "- 정보 이득은 엔트로피라는 개념을 기반으로 한다. <br>\n",
    "- 엔트로피는 주어진 데이터 집합의 혼잡도를 의미한다. <br>\n",
    "- 정보 이득 시수는 1에서 엔트로피 지수를 뺀 값으로 1 - 엔트로피 지수이다. <br>\n",
    "- 결정 트리는 이 정보 이득 지수로 분할 기준을 정한다. <br>\n",
    "- 즉 정보 이득이 높은 속성을 기준으로 분할한다. <br>\n",
    "- 지니 계수는 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수이다. <br>\n",
    "- 0이 가장 평등하고 1로 갈수록 불평등하다. <br>\n",
    "- 머신러닝에 적용될 때는 지니 계수가 낮을수록 데이터 균일도가 높은 것으로 해석, 지니 계수가 낮은 속성을 기준으로 분할한다. <br>\n",
    "\n",
    "결정 트리 알고리즘을 사이킷런에서 구현한 DecisionTreeClassifier는 기본적으로 지니 계수를 이용해 데이터 세트를 분할한다. <br>\n",
    "정보 이득이 높거나 지니 계수가 낮은 조건을 찾아서 자식 트리 노드에 걸쳐 반복적으로 분할한 뒤 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다. <br>\n",
    "\n",
    "<img src = 'image/Gini Coefficient, Data Set Segmentation Process.jpg' alt = 'Gini Coefficient, Data Set Segmentation Process' width='700' height='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc830e13",
   "metadata": {},
   "source": [
    "### 결정 트리 모델의 특징\n",
    "\n",
    "**결정 트리의 가장 큰 장점은 정보의 '균일도'라는 룰을 기반**으로 하고 있어, **알고리즘이 쉽고 직관적**이라는 것이다. <br>\n",
    "결정 트리가 룰이 매우 명확하고, 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고, 시각화 표현까지 가능하다. <br>\n",
    "**각 피처의 스케일링과 정규화 같은 전처리 작업이 필요 없다.** <br>\n",
    "가장 큰 단점은 과적합으로 인한 정확도 하락이다. <br>\n",
    "따라서 사전에 트리 크리를 제한하는 것이 오히려 성능 튜닝에 더 도움이 된다. <br>\n",
    "\n",
    "|결정 트리 장점|결정 트리 단점|\n",
    "|:------|:---|\n",
    "|쉽다. <br> 직관적이다. <br> 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않다. |과적합으로 알고리즘 성능이 떨어진다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b4032",
   "metadata": {},
   "source": [
    "### 결정 트리 파라미터 \n",
    "\n",
    "사이킷런은 결정 트리 알고리즘을 구현한 DecisionTreeClassifier와 DecisionTreeRegressor 클래스를 제공한다. <br>\n",
    "**DecisionTreeClassifier는 분류를 위한 클래스**이며, **DecisionTreeRegressor는 회귀를 위한 클래스**이다. <br>\n",
    "사이킷런의 결정 트리 구현은 **CART(Classifiercation And Regression Trees) 알고리즘 기반**이다. <br>\n",
    "\n",
    "|파라미터 이름|설명|\n",
    "|:------|:---|\n",
    "|min_sample_split|노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는데 사용된다. <br> 디폴트는 2이고 작게 설정할수록 분할되는 노드가 많아져서 과적합 가능성이 증가한다.|\n",
    "|min_sample_leaf|분할이 될 경우 왼쪽과 오른쪽 브랜치 노드에서 가져야할 최소한의 샘플 데이터 수 <br> 큰 값으로 설정될수록, 분할될 경우 왼쪽과 오른쪽의 브랜치 노드에서 가져야 할 최소한의 샘플 데이터 수 조건을 만족시키기가 어려우므로 노드 분할을 상대적으로 덜 수행한다. <br> min_sample_split과 유사하게 과적합 제어 용도, 그러나 비대칭적 데이터의 경우 특정 클래스의 데이터가 극도로 작을 수 있으므로 이 경우는 작게 설정할 필요가 있다. |\n",
    "|max_features|최적의 분할을 위해 고려할 최대 피처 갯수, 디폴트는 None으로 데이터 세트의 모든 피처를 사용해 분할을 수행한다. <br> int형으로 지정하면 대상 피처의 갯수, float형으로 지정하면 전체 피처 중 대상 피처의 퍼센트이다. <br> sqrt는 전체 피처 중 sqrt, 즉 루트 전체 피처 갯수만큼 선정한다. <br> auto로 지정하면 sqrt와 동일하다. <br> log는 전체 피처 중 log2 선정 <br> None은 전체 피처 선정이다.|\n",
    "|max_depth|트리의 최대 깊이를 규정한다. <> 디폴트는 None, None으로 설정하면 완벽하게 클래스 결정 값이 될 때까지 깊이를 계속 키우며 분할하거나 노드가 가지는 데이터 갯수가 min_sample_split보다 작아질 때까지 계속 깊이를 증가시킨다. <> 깊이가 깊어지면 min_sample_split 설정대로 최대 분할하여 과적합 할 수 있으므로 적절한 값으로 제어가 필요하다.|\n",
    "|max_leaf_nodes|말단 노드(Leaf)의 최대 갯수|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
