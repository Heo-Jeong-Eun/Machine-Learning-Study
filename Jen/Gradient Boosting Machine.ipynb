{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04523d9",
   "metadata": {},
   "source": [
    "# chapter 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59dac81",
   "metadata": {},
   "source": [
    "## Gradient Boosting Machine\n",
    "\n",
    "### GBM의 개요 및 실습\n",
    "\n",
    "**부스팅 알고리즘**은 **여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해나가면서 학습하는 방식**이다. <br>\n",
    "부스팅의 대표적인 구현은 **AdaBoost(Adaptive Boosting)** 와 **그래디언트 부스트**가 있다. <br>\n",
    "**에이다 부스트**는 오류 데이터에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘이다. <br>\n",
    "\n",
    "**GBM**은 CART 기반의 다른 알고리즘과 마찬가지로 **분류, 회귀가 가능**하다. <br>\n",
    "사이킷런은 **GBM 기반의 분류를 위해 GradientBoostingClassifier 클래스를 제공**한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e56677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처명에서 10개만 추출 :  ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']\n"
     ]
    }
   ],
   "source": [
    "# Error ! \n",
    "# 이전 Random Forest Data\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# feature.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있다. -> DataFrame으로 Load\n",
    "feature_name_df = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/features.txt', sep = '\\s+',\n",
    "                             header = None, names = ['column_index', 'column_name'])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출 : ', feature_name[:10])\n",
    "\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data = old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                 columns = ['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how = 'outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', \n",
    "                                                              'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1])\n",
    "                                                                               if x[1] > 0 else x[0], axis = 1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis = 1)\n",
    "    return new_feature_name_df\n",
    "\n",
    "def get_human_dataset():\n",
    "    # 각 데이터 파일은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    feature_name_df = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/features.txt', sep = '\\s+',\n",
    "                             header = None, names = ['column_index', 'column_name'])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 column으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    featrue_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\n",
    "    X_train = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/X_train.txt', sep = '\\s+',\n",
    "                             names = feature_name)\n",
    "    X_test = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/X_test.txt', sep = '\\s+',\n",
    "                             names = feature_name)\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\n",
    "    Y_train = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/y_train.txt', sep = '\\s+',\n",
    "                             header = None, names = ['action'])\n",
    "    Y_test = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/y_test.txt', sep = '\\s+',\n",
    "                             header = None, names = ['action'])\n",
    "    \n",
    "    # Load 된 학습 / 테스트용 DataFrame을 모두 반환\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f626256b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicate names are not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_human_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# GBM 수행 시간 측정을 위한 것이다. 시작 시간 설정\u001b[39;00m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[4], line 38\u001b[0m, in \u001b[0;36mget_human_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m featrue_name \u001b[38;5;241m=\u001b[39m new_feature_name_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/X_train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/X_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m                          names \u001b[38;5;241m=\u001b[39m feature_name)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:602\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    599\u001b[0m nrows \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# Check for duplicates in names.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[43m_validate_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:564\u001b[0m, in \u001b[0;36m_validate_names\u001b[0;34m(names)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(names)):\n\u001b[0;32m--> 564\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate names are not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    566\u001b[0m         is_list_like(names, allow_sets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(names, abc\u001b[38;5;241m.\u001b[39mKeysView)\n\u001b[1;32m    567\u001b[0m     ):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames should be an ordered collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate names are not allowed."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = get_human_dataset()\n",
    "\n",
    "# GBM 수행 시간 측정을 위한 것이다. 시작 시간 설정\n",
    "start_time = time.time()\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state = 0)\n",
    "gb_clf.fit(X_train, Y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(Y_test, gb_pred)\n",
    "\n",
    "print('GBM 정확도 : {0:.4f}'.format(gb_accuracy))\n",
    "print('GBM 수행 시간 : {0:.1f} 초'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363896e",
   "metadata": {},
   "source": [
    "특히 수행 시간 문제는 GBM이 해결해야 할 문제이다. <br>\n",
    "사이킷런의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬 처리가 지원되지 않아서 대용량 데이터의 경우 학습에 매우 많은 시간이 필요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d91565",
   "metadata": {},
   "source": [
    "### GBM 하이퍼 파라미터 \n",
    "\n",
    "- loss : 경사 하강법에서 사용할 비용 함수를 지정한다. 특별한 이유가 없으면 기본값인 deviance를 그대로 적용한다. <br>\n",
    "- learning_rate : GBM이 학습을 진행할 때마다 적용하는 학습률이다. <br>\n",
    "- n_estimators : weak learner의 갯수이다. <br>\n",
    "- subsample : weak learner가 학습에 사용하는 데이터의 샘플링 비율이다. 기본값은 1이고 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
