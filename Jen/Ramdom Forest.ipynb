{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44bd9bf",
   "metadata": {},
   "source": [
    "# chapter 4. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f8549",
   "metadata": {},
   "source": [
    "## Ramdom Forest\n",
    "\n",
    "### 랜덤 포레스트의 개요 및 실습\n",
    "\n",
    "배깅의 대표적인 알고리즘은 **랜덤 포레스트**이다. <br>\n",
    "앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있으며 다양한 영역에서 높은 예측 성능을 보인다. <br>\n",
    "랜덤 포레스트의 기반 알고리즘은 결 트리로서 쉽고 직관적인 장점을 가지고 있다. <br>\n",
    "\n",
    "**랜덤 포레스트**는 **여러 개의 결정 트리 분류리가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정**을 하게 된다. \n",
    "\n",
    "<img src = 'image/Random Forest.jpg' alt = 'Random Forest' width='600' height='600'>\n",
    "\n",
    "랜덤 포레스트의 개별적인 분류기의 기반 알고리즘은 결정 트리이지만 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트이다. <br>\n",
    "**여러 개의 데이터 세트를 중첩되게 분리하는 것**을 **부스트트래핑 분할 방식**이라고 한다. <br>\n",
    "데이터가 중첩된 개별 데이터 세트에 결정 트리 분류기를 각각 적용하는 것이 랜덤 포레스트이다. <br>\n",
    "사이킷런은 **RandomForestClassifier 클래스**를 통해 **랜덤 포레스트 기반의 분류를 지원**한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b681489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 피처명에서 10개만 추출 :  ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Duplicate names are not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Load 된 학습 / 테스트용 DataFrame을 모두 반환\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, Y_train, Y_test\n\u001b[0;32m---> 52\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_human_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mget_human_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m featrue_name \u001b[38;5;241m=\u001b[39m new_feature_name_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/X_train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/X_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m                          names \u001b[38;5;241m=\u001b[39m feature_name)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:602\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    599\u001b[0m nrows \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# Check for duplicates in names.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[43m_validate_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:564\u001b[0m, in \u001b[0;36m_validate_names\u001b[0;34m(names)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(names)):\n\u001b[0;32m--> 564\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate names are not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    566\u001b[0m         is_list_like(names, allow_sets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(names, abc\u001b[38;5;241m.\u001b[39mKeysView)\n\u001b[1;32m    567\u001b[0m     ):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames should be an ordered collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate names are not allowed."
     ]
    }
   ],
   "source": [
    "# Error ! \n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# feature.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있다. -> DataFrame으로 Load\n",
    "feature_name_df = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/features.txt', sep = '\\s+',\n",
    "                             header = None, names = ['column_index', 'column_name'])\n",
    "\n",
    "# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출 : ', feature_name[:10])\n",
    "\n",
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data = old_feature_name_df.groupby('column_name').cumcount(),\n",
    "                                 columns = ['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how = 'outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', \n",
    "                                                              'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1])\n",
    "                                                                               if x[1] > 0 else x[0], axis = 1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis = 1)\n",
    "    return new_feature_name_df\n",
    "\n",
    "def get_human_dataset():\n",
    "    # 각 데이터 파일은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당\n",
    "    feature_name_df = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/features.txt', sep = '\\s+',\n",
    "                             header = None, names = ['column_index', 'column_name'])\n",
    "    \n",
    "    # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 column으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    featrue_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\n",
    "    X_train = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/X_train.txt', sep = '\\s+',\n",
    "                             names = feature_name)\n",
    "    X_test = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/X_test.txt', sep = '\\s+',\n",
    "                             names = feature_name)\n",
    "    \n",
    "    # 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\n",
    "    Y_train = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/y_train.txt', sep = '\\s+',\n",
    "                             header = None, names = ['action'])\n",
    "    Y_test = pd.read_csv('/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/y_test.txt', sep = '\\s+',\n",
    "                             header = None, names = ['action'])\n",
    "    \n",
    "    # Load 된 학습 / 테스트용 DataFrame을 모두 반환\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = get_human_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f85225",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Duplicate names are not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 결정 트리에서 사용한 get_human_dataset()을 이용해 학습 / 테스트용 DataFrame 반환\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m X_train, X_test, Y_train, Y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_human_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 랜덤 포레스트 학습 및 별도의 테스트 세트로 예측 성능 평가\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rf_clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mget_human_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m featrue_name \u001b[38;5;241m=\u001b[39m new_feature_name_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/1001l1000/Documents/AI/Jen/data/human_activity/train/X_train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/1001l1000/Documents/AI/Jen/data/human_activity/test/X_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     41\u001b[0m                          names \u001b[38;5;241m=\u001b[39m feature_name)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 학습 피처 데이터세트와 테스트 피처 데이터를 DataFrame으로 Loading, column 이름은 featrue_name 적용\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:602\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    599\u001b[0m nrows \u001b[38;5;241m=\u001b[39m kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# Check for duplicates in names.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[43m_validate_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/pandas/io/parsers/readers.py:564\u001b[0m, in \u001b[0;36m_validate_names\u001b[0;34m(names)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(names)):\n\u001b[0;32m--> 564\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate names are not allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    566\u001b[0m         is_list_like(names, allow_sets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(names, abc\u001b[38;5;241m.\u001b[39mKeysView)\n\u001b[1;32m    567\u001b[0m     ):\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames should be an ordered collection.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Duplicate names are not allowed."
     ]
    }
   ],
   "source": [
    "# Error ! \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 결정 트리에서 사용한 get_human_dataset()을 이용해 학습 / 테스트용 DataFrame 반환\n",
    "X_train, X_test, Y_train, Y_test = get_human_dataset()\n",
    "\n",
    "# 랜덤 포레스트 학습 및 별도의 테스트 세트로 예측 성능 평가\n",
    "rf_clf = RandomForestClassifier(random_state = 0, max_depth = 8)\n",
    "rf_clf.fit(X_train, Y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, pred)\n",
    "print('랜덤 포레스트 정확도 : {0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c7d15f",
   "metadata": {},
   "source": [
    "### 랜덤 포레스트 하이퍼 파라미터 및 튜닝\n",
    "\n",
    "트리 기반 앙상블의 단점은 하이퍼 파라미터가 너무 많고, 이로 인한 시간이 많이 소모된다는 점이다. <br>\n",
    "트리 기반 자체의 하이퍼 파라미터가 원래 많은데 배깅, 부스팅, 학습, 정규화 등을 위한 하이퍼 파라미터까지 추가되므로 일반적으로 다른 ML 알고리즘에 비해 많을 수 밖에 없다. <br>\n",
    "랜덤 포레스트가 적은 편에 속하는데, 결정 트리에서 사용되는 하이퍼 마라미터와 같은 파라미터가 대부분이기 때문이다. <br>\n",
    "\n",
    "- n_estimators : 랜덤 포레스트에서 결정 트리의 개수를 지정한다. 디폴트는 10개이다. 많이 설정할 수록 좋은 성능을 기대할 수 있지만 계속 증가시킨다고 성능이 향상되는 것은 아니다. 또한 늘릴수록 학습 수행 시간이 오래걸리는 것도 감안해야 한다. \n",
    "- max_feature : 결정 트리에서 사용된 max_feature 파라미터와 같다. 하지만 RandomForestClassifier의 기본 max_feature는 'None'이 아니라 'auto'. 즉 'sqrt'와 같다. 따라서 랜덤 포레스트의 트리를 분할하는 피처를 참조할 때 전체 피처가 아니라 sqrt만큼 참조한다. \n",
    "- max_depth나 min_samples_leat, min_sample_split와 같이 결정 트리에서 과적합을 개선하기 위해 사용되는 파라미터가 랜덤 포레스트에도 공통적으로 적용된다. \n",
    "\n",
    "랜덤 포레스트는 CPU 병렬 처리도 효과적으로 수행되어 빠른 학습이 가능하기 때문에 뒤에 소개할 그래디언트 부스팅보다 예측 성능이 약간 떨어지기라도 랜덤 포레스트로 일반 기반 모델을 먼저 구축하는 경우가 많다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36886107",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m rf_clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m grid_cv \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_clf, param_grid \u001b[38;5;241m=\u001b[39m params, cv \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m grid_cv\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, Y_train)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m최적 하이퍼 파라미터 : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, grid_cv\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m최고 예측 정확도 : \u001b[39m\u001b[38;5;132;01m{0:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(grid_cv,best_score_))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth' : [8, 16, 24],\n",
    "    'min_samples_leaf' : [1, 6, 12], \n",
    "    'min_samples_split' : [2, 8, 16]\n",
    "}\n",
    "\n",
    "# RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
    "rf_clf = RandomForestClassifier(n_estimators = 100, random_state = 0, n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 2, n_jobs = -1)\n",
    "grid_cv.fit(X_train, Y_train)\n",
    "\n",
    "print('최적 하이퍼 파라미터 : \\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도 : {0:.4f}'.format(grid_cv,best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aba6215d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m rf_clf1 \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, min_samples_leaf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                 min_samples_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m rf_clf1\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, Y_train)\n\u001b[1;32m      4\u001b[0m pred \u001b[38;5;241m=\u001b[39m rf_clf1\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m에측 정확도 : \u001b[39m\u001b[38;5;132;01m{0:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(accuracy_score(Y_test, pred)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "rf_clf1 = RandomForestClassifier(n_estimators = 100, min_samples_leaf = 6, max_depth = 16,\n",
    "                                min_samples_split = 2, random_state = 0)\n",
    "rf_clf1.fit(X_train, Y_train)\n",
    "pred = rf_clf1.predict(X_test)\n",
    "print('에측 정확도 : {0:.4f}'.format(accuracy_score(Y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46da8299",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m ftr_importances_value \u001b[38;5;241m=\u001b[39m \u001b[43mrf_clf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importances_\u001b[49m\n\u001b[1;32m      6\u001b[0m ftr_importances \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(ftr_importances_values, index \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m      7\u001b[0m ftr_top20 \u001b[38;5;241m=\u001b[39m ftr_importances\u001b[38;5;241m.\u001b[39msort_valeus(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)[:\u001b[38;5;241m20\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:628\u001b[0m, in \u001b[0;36mBaseForest.feature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_importances_\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m    The impurity-based feature importances.\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;124;03m        array of zeros.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m     all_importances \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[1;32m    631\u001b[0m         delayed(\u001b[38;5;28mgetattr\u001b[39m)(tree, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tree\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mnode_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    634\u001b[0m     )\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_importances:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/J/lib/python3.9/site-packages/sklearn/utils/validation.py:1390\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1385\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1386\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m     ]\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[0;32m-> 1390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "ftr_importances_value = rf_clf1.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values, index = X_train.columns)\n",
    "ftr_top20 = ftr_importances.sort_valeus(ascending = False)[:20]\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.title('Feature importance Top 20')\n",
    "sns.barplot(x = ftr_top20, y = ftr_top20.index)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
